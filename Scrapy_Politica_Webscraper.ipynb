{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrapy Politica Webscraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+HnqMRR3n+FgbF8Uxa3Es",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bcopeland64/Omdena-Projects/blob/main/Scrapy_Politica_Webscraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErIgWMIP0aqd",
        "outputId": "40e26fa1-d90e-4795-8b70-21c6892eb234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rtZHXfX91PMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code will extract information from the designated webpages\n",
        "\n",
        "page_grab_number = 5\n",
        "\n",
        "upperframe = []\n",
        "for page in range(1, page_grab_number+1):\n",
        "  print('downloading page: ', page)\n",
        "  url = 'https://www.semana.com/buscador/?query=politica'+str(page)\n",
        "\n",
        "  try:\n",
        "    requests.get(url)\n",
        "  except Exception as e:\n",
        "    error_type, error_obj, error_info = sys.exec_info()\n",
        "    print('Error for link: ', url)\n",
        "    print(error_type, 'line: ', error_info.tb_lineno)\n",
        "    continue\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    frame = []\n",
        "\n",
        "    links = soup.find_all('li', attrs={'class':'o-listicle_item'})\n",
        "    print(len(links))\n",
        "\n",
        "    filename = 'scraped.csv'\n",
        "    f = open(filename, 'w')\n",
        "    headers = 'Statement, Link, Date, Source, label \\n'\n",
        "    f.write(headers)\n",
        "\n",
        "    for i in links:\n",
        "      Statement = i.find('div', attrs={'class':'m-statement_quote'}).text\n",
        "      Link = i.find('div', attrs={'class':'m-statement_quote'}).find('a')[href].strip()\n",
        "      Date = i.find('div', attrs={'class':'m-statement_body'}).find('footer').text[-14:-1].strip()\n",
        "      Source = i.find('div', attrs={'class':'m-statement_author'}).find('a').text.strip()\n",
        "      label = i.find('div', attrs={'class':'m-statement_content'}).find('img', attrs={'class':'c-image_original'}).get('alt').strip()\n",
        "\n",
        "      frame.append(Statement, Link, Date, Source, label)\n",
        "      f.write(Statement.replace(',','^')+','+Link.replace(',', '^'+','+Date+','+Source.replace(',', '^')+','+label.replace(',', '^')\n",
        "\n",
        "    upperframe.extend(frame)\n",
        "    \n",
        "f.close\n",
        "  \n",
        "\n",
        "data = pd.DataFrame(frame_2, columns=['Statement', 'Link', 'Date', 'Source', 'label'])\n",
        "data.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tl-xFxU21dbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0izBPliK1zUS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}